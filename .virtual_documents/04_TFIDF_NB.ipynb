


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
# TODO more model types can be imported here if interesting

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# we may choose to only optimize for Tfidf bc this structures the data with more information 
from nltk.corpus import stopwords


# Read in data
posts = pd.read_csv('./data/posts_cleaned.csv')
posts.head()


X = posts['all_text']
y = posts['subreddit']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


# Set up a pipeline up with two stages:
# 1. tf-idf vectorizer (transformer)
# 2. Multinomial Naive Bayes (estimator)

pipe = Pipeline([
    ('tvec', TfidfVectorizer()),
    ('nb', MultinomialNB())
])

params = {
    'tvec__stop_words': [None, 'english'],
    'tvec__min_df': [1, 2, 3],
    'tvec__ngram_range': [(1, 1), (1, 2)],
    'tvec__max_features': [2000, 3000, 4000, 5000, None]
}

gs = GridSearchCV(pipe, params, n_jobs=8)


%%time
gs.fit(X_train, y_train)


gs.score(X_train, y_train)


gs.score(X_test, y_test)





pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False).iloc[:5, 4:]





gs.best_params_


gs.best_estimator_[0].get_feature_names_out()[-10:]


posts[posts['all_text'].str.lower().str.contains('zealand')]


posts['all_text'] # .str.lower()





# make the features from some tokenizer strategy
# fit to logreg
# examine score
