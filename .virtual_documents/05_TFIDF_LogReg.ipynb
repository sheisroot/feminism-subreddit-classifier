


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
# TODO more model types can be imported here if interesting

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# we may choose to only optimize for Tfidf bc this structures the data with more information 
from nltk.corpus import stopwords


# Read in data
posts = pd.read_csv('./data/posts_cleaned.csv')
posts.head()


X = posts['all_text']
y = posts['subreddit']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


# Set up a pipeline up with two stages:
# 1. tf-idf vectorizer (transformer)
    # optimize parameters for this tokenizer strategy
# 2. LogisticRegression
    # optimize logistic regression params for this classification strategy

pipe = Pipeline([
    ('tvec', TfidfVectorizer()),
    ('logreg', LogisticRegression())
])

params = {
    'tvec__stop_words': [None],
    'tvec__min_df': [2],
    'tvec__ngram_range': [(1, 2)],
    'tvec__max_features': [5000],
    'logreg__C': np.logspace(0, 1, 10),
    'logreg__penalty': ['l1', 'l2'],
    'logreg__solver':['liblinear'],
    
}

gs = GridSearchCV(pipe, params, n_jobs=4)


%%time
gs.fit(X_train, y_train)


gs.score(X_test, y_test)


gs.score(X_train, y_train)





gs.best_params_


pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False).iloc[:5, 4:]





gs.best_params_


gs.best_estimator_[0].get_feature_names_out()[-10:]


posts[posts['all_text'].str.lower().str.contains('zealand')]


posts['all_text'] # .str.lower()





# make the features from some tokenizer strategy
# fit to logreg
# examine score
