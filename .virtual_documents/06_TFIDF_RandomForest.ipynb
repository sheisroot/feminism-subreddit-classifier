


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.tree import DecisionTreeClassifier, plot_tree

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# we may choose to only optimize for Tfidf bc this structures the data with more information 
from nltk.corpus import stopwords


# Read in data
posts = pd.read_csv('./data/posts_cleaned.csv')
posts.head()


X = posts['all_text']
y = posts['subreddit']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


# Set up a pipeline up with two stages:
# 1. tf-idf vectorizer (transformer)
    # optimize parameters for this tokenizer strategy
# 2. Decision Tree classifier
    

pipe = Pipeline([
    ('tvec', TfidfVectorizer()),
    ('tree', DecisionTreeClassifier()),
])

params = {
    'tvec__stop_words': [None],
    'tvec__min_df': [2],
    'tvec__ngram_range': [(1, 2)],
    'tvec__max_features': [5000],
    'tree__max_depth': [3, 5],
    'tree__min_samples_split': [3, 5, 8],    
}

gs = GridSearchCV(pipe, params, n_jobs=4)


%%time
gs.fit(X_train, y_train)


gs.score(X_train, y_train)


gs.score(X_test, y_test)





gs.best_params_


my_features_importances = gs.best_estimator_[1].feature_importances_
tree_table = pd.DataFrame({
    'variable': gs.best_estimator_[0].get_feature_names_out(),
    'importance': my_features_importances,
}).sort_values('importance', ascending=False)
tree_table[:10]


# Visualize the decision tree
tree = gs.best_estimator_[1]
plt.figure(figsize=(18, 10))
plot_tree(
    tree,
    feature_names=tree_table['variable'],
    class_names=['MensRights', 'TwoXChromosomes'],
    filled=True
)
plt.savefig('./images/tree.png')
;

    


pd.DataFrame(gs.cv_results_).sort_values('mean_test_score', ascending=False).iloc[:5, 4:]





gs.best_params_


gs.best_estimator_[0].get_feature_names_out()[-10:]


posts[posts['all_text'].str.lower().str.contains('zealand')]


posts['all_text'] # .str.lower()





# make the features from some tokenizer strategy
# fit to logreg
# examine score
