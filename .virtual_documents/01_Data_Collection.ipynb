


# import numpy as np
import pandas as pd
# import matplotlib.pyplot as plt

import praw



# Credentials
reddit = praw.Reddit(
    client_id='E_FrurAX47RmpJvrUl-gjA',
    client_secret='XOBwQG1wO3WG3XqOO4PXqSlbfnfcnw',
    user_agent='Faye-dune-popcorn-bucket',
    username='New-Veterinarian744',
    password='e2-5Jns9TBg*Y.F'
)


# Multiple subreddits
two_x = 'TwoXChromosomes'
mrm = 'MensRights'

subreddit_two_x = reddit.subreddit(two_x)
subreddit_mrm = reddit.subreddit(mrm)


# Max 100 posts per request
# Make multiple calls with wait times (e.g. sleep)
# ROWAN did I handle server rate limits correctly
posts_new = subreddit_two_x.new(limit=1000)
posts_top = subreddit_two_x.top(limit=1000)
posts_hot = subreddit_two_x.hot(limit=1000)
posts_contr = subreddit_two_x.controversial(limit=1000)


posts_new = subreddit_mrm.new(limit=1000)
posts_top = subreddit_mrm.top(limit=1000)
posts_hot = subreddit_mrm.hot(limit=1000)
posts_contr = subreddit_mrm.controversial(limit=1000)


# data = {} # Try collecting as a set to throw out duplicates
# for post in posts_new:
#     data.add([post.created_utc, post.title, post.selftext, post.subreddit])

# # Turn into a dataframe
# df_reddit = pd.DataFrame(data, columns = ['created_utc', 'title', 'self_text', 'subreddit'])



# ROWAN should we collect comments as well
# refactor code to move stuff into this def

def collect_posts(my_posts, data_set):
    for post in my_posts:
        data.append([post.created_utc, post.title, post.selftext, post.subreddit])


data = []
posts_list = [posts_new, posts_hot, posts_top, posts_contr]

for p in posts_list:
    collect_posts(p, data)


df_reddit = pd.DataFrame(data, columns = ['created_utc', 'title', 'self_text', 'subreddit'])
df_reddit.head()



# check for duplicates in df


df_reddit.shape


reduced = df_reddit.drop_duplicates(subset=['created_utc', 'title'])
reduced.shape


reduced.to_csv('./data/two_x.csv', index=False)


reduced.to_csv('./data/mrm.csv', index=False)





# can possibly scrape more data by specifying filter keywords
# but it may create underlying correlations
# can set a time_filter to slightly change the time bounds 'month', 'week', 'year', default='all'




# time_filter
# posts_new = subreddit_two_x.new(limit=1000)
# posts_top = subreddit_two_x.top(limit=1000)
# posts_hot = subreddit_two_x.hot(limit=1000)
posts_contr = subreddit_two_x.controversial(time_filter='year', limit=1000)

# rising, random_rising, search
# posts_





# ROWAN how would we use automation/pipelines to optimize data collection
# could move this code to a .py script and specify a cron job



